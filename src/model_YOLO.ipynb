{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wde4FLAKW_NH"
      },
      "source": [
        "## Install\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQOYipF9YoXV"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aoQDuNMKYqE-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import textwrap\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pickle\n",
        "import torch\n",
        "from ultralytics.data.dataset import YOLODataset\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from ultralytics import YOLO\n",
        "import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xSU8cSSe0Kk"
      },
      "source": [
        "## Computing class weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o8TbDd6wZCc2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "Finding Label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "count",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "f89d916f-e343-471c-a79a-e3bfd9f621d7",
              "rows": [
                [
                  "Atelectasis",
                  "153"
                ],
                [
                  "Effusion",
                  "130"
                ],
                [
                  "Cardiomegaly",
                  "124"
                ],
                [
                  "Infiltrate",
                  "105"
                ],
                [
                  "Pneumonia",
                  "102"
                ],
                [
                  "Pneumothorax",
                  "83"
                ],
                [
                  "Mass",
                  "72"
                ],
                [
                  "Nodule",
                  "67"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 8
              }
            },
            "text/plain": [
              "Finding Label\n",
              "Atelectasis     153\n",
              "Effusion        130\n",
              "Cardiomegaly    124\n",
              "Infiltrate      105\n",
              "Pneumonia       102\n",
              "Pneumothorax     83\n",
              "Mass             72\n",
              "Nodule           67\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: {'Atelectasis': 0.68, 'Effusion': 0.8, 'Cardiomegaly': 0.84, 'Infiltrate': 1.0, 'Pneumonia': 1.02, 'Pneumothorax': 1.26, 'Mass': 1.45, 'Nodule': 1.56}\n"
          ]
        }
      ],
      "source": [
        "# Defining the file paths\n",
        "current_dir = os.getcwd()\n",
        "current_dir = os.path.dirname(current_dir)  # Get the current working directory\n",
        "\n",
        "CSV = f\"{current_dir}/data/train.csv\"  # CSV metadata file\n",
        "IMAGES = f\"{current_dir}/train\"  # Folder containing training images\n",
        "SPLITDIR = \"splits\"  # Directory to store train/val/test splits\n",
        "os.makedirs(SPLITDIR, exist_ok=True)\n",
        "\n",
        "# Loading the raw dataset\n",
        "raw = pd.read_csv(CSV)\n",
        "\n",
        "# Light cleaning: we remove useless whitespaces and empty labels\n",
        "raw[\"Finding Label\"] = raw[\"Finding Label\"].str.strip()\n",
        "raw = raw[raw[\"Finding Label\"].notna()]\n",
        "\n",
        "# Counting samples per class\n",
        "cls_counts = raw[\"Finding Label\"].value_counts().sort_values(ascending=False)\n",
        "display(cls_counts)\n",
        "\n",
        "# We compute here simple class weights (inverse frequency scaling)\n",
        "tot = cls_counts.sum()\n",
        "class_weights = (tot / (len(cls_counts) * cls_counts)).round(2).tolist()\n",
        "\n",
        "print(\n",
        "    \"Class weights:\", dict(zip(cls_counts.index, class_weights, strict=True))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGjYzpdve01e"
      },
      "source": [
        "## Creating training/validation stratified splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ig0pAHbaeytj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "608 training samples – 152 validation samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_256102/435476801.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(\n"
          ]
        }
      ],
      "source": [
        "# random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# Group annotations (bounding boxes + labels) per image\n",
        "grouped = (\n",
        "    raw.groupby(\"Image Index\")\n",
        "    .apply(\n",
        "        lambda g: {\n",
        "            \"labels\": g[\"Finding Label\"].tolist(),\n",
        "            \"bboxes\": g[[\"Bbox [x\", \"y\", \"w\", \"h]\"]].values.tolist(),\n",
        "        }\n",
        "    )\n",
        "    .reset_index()\n",
        "    .rename(columns={0: \"annotation\"})\n",
        ")\n",
        "\n",
        "# Create a column for stratified splitting (use the first label as stratum)\n",
        "grouped[\"strat\"] = grouped[\"annotation\"].apply(lambda d: d[\"labels\"][0])\n",
        "\n",
        "# Stratified train/val split (80/20)\n",
        "train_df, val_df = train_test_split(\n",
        "    grouped, test_size=0.2, random_state=42, stratify=grouped[\"strat\"]\n",
        ")\n",
        "\n",
        "# Saving splits\n",
        "train_df.to_pickle(f\"{SPLITDIR}/train_split.pkl\")\n",
        "val_df.to_pickle(f\"{SPLITDIR}/val_split.pkl\")\n",
        "\n",
        "print(len(train_df), \"training samples –\", len(val_df), \"validation samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVVryHMR4J8h"
      },
      "source": [
        "## Converting our dataset to YOLO format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g0ORHqLIh_7Y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 608/608 [00:00<00:00, 6558.86it/s]\n",
            "100%|██████████| 152/152 [00:00<00:00, 8202.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion complete.\n",
            "\n",
            "path: /home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/src/yolo_cxr\n",
            "train: images/train\n",
            "val: images/val\n",
            "names: ['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltrate', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Setting-up YOLO directory structure\n",
        "YOLO_ROOT = \"yolo_cxr\"\n",
        "IMG_TRAIN = os.path.join(YOLO_ROOT, \"images\", \"train\")\n",
        "IMG_VAL = f\"{YOLO_ROOT}/images/val\"\n",
        "LAB_TRAIN = f\"{YOLO_ROOT}/labels/train\"\n",
        "LAB_VAL = f\"{YOLO_ROOT}/labels/val\"\n",
        "\n",
        "\n",
        "for path in [IMG_TRAIN, IMG_VAL, LAB_TRAIN, LAB_VAL]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# Extracting all unique classes and build class-to-index mapping\n",
        "CLASSES = sorted(\n",
        "    {label for ann in grouped[\"annotation\"] for label in ann[\"labels\"]}\n",
        ")\n",
        "cls2idx = {class_name: idx for idx, class_name in enumerate(CLASSES)}\n",
        "\n",
        "\n",
        "# Conversion function: from our basic style to YOLOv5 format\n",
        "def _convert(row, split):\n",
        "    image_name = row[\"Image Index\"]\n",
        "    src_path = os.path.join(IMAGES, image_name)\n",
        "\n",
        "    # Choose target image and label path based on split\n",
        "    dst_img_path = os.path.join(\n",
        "        IMG_TRAIN if split == \"train\" else IMG_VAL, image_name\n",
        "    )\n",
        "    dst_label_path = os.path.join(\n",
        "        LAB_TRAIN if split == \"train\" else LAB_VAL,\n",
        "        image_name.replace(\".png\", \".txt\"),\n",
        "    )\n",
        "\n",
        "    # Copy image if not already copied\n",
        "    if not os.path.exists(dst_img_path):\n",
        "        shutil.copy2(src_path, dst_img_path)\n",
        "\n",
        "    # We create here the YOLO-format\n",
        "    w, h = Image.open(src_path).size\n",
        "    with open(dst_label_path, \"w\") as f:\n",
        "        for bbox, label in zip(\n",
        "            row[\"annotation\"][\"bboxes\"],\n",
        "            row[\"annotation\"][\"labels\"],\n",
        "            strict=True,\n",
        "        ):\n",
        "            x, y, w_box, h_box = bbox\n",
        "            x_center = (x + w_box / 2) / w\n",
        "            y_center = (y + h_box / 2) / h\n",
        "            norm_w = w_box / w\n",
        "            norm_h = h_box / h\n",
        "            f.write(\n",
        "                f\"{cls2idx[label]} {x_center:.6f} {y_center:.6f} {norm_w:.6f} {norm_h:.6f}\\n\"\n",
        "            )\n",
        "\n",
        "\n",
        "# We then convert all images and annotations in the right format for YOLO\n",
        "for _, row in tqdm.tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    _convert(row, split=\"train\")\n",
        "\n",
        "for _, row in tqdm.tqdm(val_df.iterrows(), total=len(val_df)):\n",
        "    _convert(row, split=\"val\")\n",
        "\n",
        "print(\"Conversion complete.\")\n",
        "\n",
        "\n",
        "# Here we write YOLO dataset config YAML file (required by YOLO)\n",
        "yaml_content = textwrap.dedent(f\"\"\"\n",
        "    path: {os.path.abspath(YOLO_ROOT)}\n",
        "    train: images/train\n",
        "    val: images/val\n",
        "    names: {CLASSES}\n",
        "\"\"\")\n",
        "\n",
        "with open(f\"{SPLITDIR}/chest.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(yaml_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNUPV_wk4UcI"
      },
      "source": [
        "## Dealing with rare classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gB5E6BCm7Bx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 836/836 [00:03<00:00, 266.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Patch bank created with 375 rare-class samples.\n"
          ]
        }
      ],
      "source": [
        "# Defining rare classes to extract patches from (based on EDA)\n",
        "RARE_CLASSES = {\"Pneumothorax\", \"Mass\", \"Nodule\"}\n",
        "PATCH_BANK = \"splits/rare_patches.pkl\"\n",
        "MARGIN = 4  # pixels of padding around each box for context\n",
        "\n",
        "# We build here patch bank only if it doesn't already exist\n",
        "if not os.path.exists(PATCH_BANK):\n",
        "    patches = []\n",
        "\n",
        "    # Iterating over all annotated samples\n",
        "    for _, row in tqdm.tqdm(raw.iterrows(), total=len(raw)):\n",
        "        label = row[\"Finding Label\"]\n",
        "        if label not in RARE_CLASSES:\n",
        "            continue  # Skip non-rare classes\n",
        "\n",
        "        # We load the image and extract the bounding box coordinates with margin\n",
        "        img_path = os.path.join(IMAGES, row[\"Image Index\"])\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        x, y, w_box, h_box = row[[\"Bbox [x\", \"y\", \"w\", \"h]\"]]\n",
        "        x1 = max(int(x - MARGIN), 0)\n",
        "        y1 = max(int(y - MARGIN), 0)\n",
        "        x2 = min(int(x + w_box + MARGIN), img.shape[1])\n",
        "        y2 = min(int(y + h_box + MARGIN), img.shape[0])\n",
        "\n",
        "        # We crop the patch and store it with its label\n",
        "        patch = img[y1:y2, x1:x2]\n",
        "        patches.append({\"patch\": patch, \"label\": label})\n",
        "\n",
        "    with open(PATCH_BANK, \"wb\") as f:\n",
        "        pickle.dump(patches, f)\n",
        "\n",
        "    print(f\" Patch bank created with {len(patches)} rare-class samples.\")\n",
        "else:\n",
        "    print(\" Patch bank already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LGqm3lDNq8Jo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RareCopyPasteDataset enabled\n"
          ]
        }
      ],
      "source": [
        "# We load the rare class patch bank (previously saved with pickle)\n",
        "with open(\"splits/rare_patches.pkl\", \"rb\") as f:\n",
        "    _rare_bank = pickle.load(f)\n",
        "\n",
        "# We get the class indices of rare classes\n",
        "_rare_cls_idx = {cls2idx[c] for c in RARE_CLASSES}\n",
        "\n",
        "\n",
        "# Custom dataset class that applies Copy-Paste augmentation with rare class patches\n",
        "class RareCopyPasteDataset(YOLODataset):\n",
        "    def __init__(self, *args, p_paste=0.8, max_patches=3, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.p_paste = p_paste  # Probability of applying copy-paste\n",
        "        self.max_patches = (\n",
        "            max_patches  # Max number of patches to paste per image\n",
        "        )\n",
        "\n",
        "    def _paste(self, img, patch):\n",
        "        # Paste a patch at a random location within the image (with some margin)\n",
        "        H, W = img.shape[:2]\n",
        "        ph, pw = patch.shape[:2]\n",
        "        x = random.randint(int(0.15 * W), max(int(0.85 * W - pw), 1))\n",
        "        y = random.randint(int(0.15 * H), max(int(0.85 * H - ph), 1))\n",
        "        img[y : y + ph, x : x + pw] = patch\n",
        "        return x, y, pw, ph\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target, _, _ = super().__getitem__(idx)\n",
        "\n",
        "        # We skip copy-paste if the image already contains a rare class\n",
        "        if any(int(c) in _rare_cls_idx for c in target[\"cls\"]):\n",
        "            return img, target, img.copy(), img.copy()\n",
        "\n",
        "        # With probability p_paste, paste up to max_patches rare class objects\n",
        "        if random.random() < self.p_paste:\n",
        "            for _ in range(random.randint(1, self.max_patches)):\n",
        "                entry = random.choice(_rare_bank)\n",
        "                x, y, w, h = self._paste(img, entry[\"patch\"])\n",
        "                target[\"bboxes\"].append([x, y, w, h])\n",
        "                target[\"cls\"].append(torch.tensor(cls2idx[entry[\"label\"]]))\n",
        "\n",
        "        return img, target, img.copy(), img.copy()\n",
        "\n",
        "\n",
        "# Monkey-patch the Ultralytics dataset builder with our custom dataset\n",
        "from ultralytics.data import build\n",
        "\n",
        "build.YOLODataset = RareCopyPasteDataset\n",
        "\n",
        "print(\"RareCopyPasteDataset enabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZThjNgfm4Z8Q"
      },
      "source": [
        "## Training of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mm9z0hJhsjey"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.161 🚀 Python-3.11.10 torch-2.7.0+cu126 CPU (AMD Ryzen 7 PRO 7840U w/ Radeon 780M Graphics)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=10.0, cache=False, cfg=None, classes=None, close_mosaic=50, cls=1.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=splits/chest.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0, hsv_s=0, hsv_v=0, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.002, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=0.25, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=70, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.3, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2119144  ultralytics.nn.modules.head.Detect           [8, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,138,696 parameters, 11,138,680 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.9.cv1.conv.weight'\n",
            "Freezing layer 'model.9.cv1.bn.weight'\n",
            "Freezing layer 'model.9.cv1.bn.bias'\n",
            "Freezing layer 'model.9.cv2.conv.weight'\n",
            "Freezing layer 'model.9.cv2.bn.weight'\n",
            "Freezing layer 'model.9.cv2.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 4941.7±1759.6 MB/s, size: 400.8 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Cha\u001b[0m\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 6411.1±984.3 MB/s, size: 388.4 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Chall\u001b[0m\n",
            "/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.002, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/30         0G      4.268      80.75      3.309          4       1024:  ^C\n"
          ]
        }
      ],
      "source": [
        "!yolo detect train \\\n",
        "    data=\"splits/chest.yaml\" \\\n",
        "    model=yolov8s.pt \\\n",
        "    imgsz=1024 \\\n",
        "    batch=4 \\\n",
        "    epochs=30 \\\n",
        "    freeze=10 \\\n",
        "    patience=70 \\\n",
        "    close_mosaic=50 \\\n",
        "    mosaic=0.25 \\\n",
        "    scale=0.3 \\\n",
        "    cls=1.5 \\\n",
        "    box=10.0 \\\n",
        "    dfl=1.5 \\\n",
        "    hsv_h=0 \\\n",
        "    hsv_s=0 \\\n",
        "    hsv_v=0 \\\n",
        "    lr0=0.002 \\\n",
        "    optimizer=SGD \\\n",
        "    seed=42 \\\n",
        "    amp=True \\\n",
        "    project=\"runs/detect\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cuK7tMLpsn9w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 956, in entrypoint\n",
            "    model = YOLO(model, task=task)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/models/yolo/model.py\", line 79, in __init__\n",
            "    super().__init__(model=model, task=task, verbose=verbose)\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 151, in __init__\n",
            "    self._load(model, task=task)\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 295, in _load\n",
            "    self.model, self.ckpt = attempt_load_one_weight(weights)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 1548, in attempt_load_one_weight\n",
            "    ckpt, weight = torch_safe_load(weight)  # load ckpt\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 1446, in torch_safe_load\n",
            "    ckpt = torch.load(file, map_location=\"cpu\")\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/utils/patches.py\", line 119, in torch_load\n",
            "    return _torch_load(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/serialization.py\", line 1479, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/serialization.py\", line 759, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/serialization.py\", line 740, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'runs/detect/yolo8x_cxr_v2_res1024_f1/weights/last.pt'\n"
          ]
        }
      ],
      "source": [
        "# Second part of the training, without freezing any layers\n",
        "!yolo detect train \\\n",
        "    model=runs/detect/yolo8x_cxr_v2_res1024_f1/weights/last.pt \\\n",
        "    data=splits/chest.yaml \\\n",
        "    epochs=70 \\\n",
        "    freeze=0 \\\n",
        "    lr0=0.0005 \\\n",
        "    patience=70 \\\n",
        "    imgsz=1024 \\\n",
        "    batch=4 \\\n",
        "    project=runs/detect \\\n",
        "    name=yolo8x_cxr_v2_res1024_f2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paifufz14fHv"
      },
      "source": [
        "## Validation of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9XCu1i-xBk4"
      },
      "outputs": [],
      "source": [
        "# Quick validation on the validation set using best model weights\n",
        "!yolo val \\\n",
        "    model=runs/detect/yolo8x_cxr_v2_res1024_f2/weights/best.pt \\\n",
        "    data=splits/chest.yaml \\\n",
        "    imgsz=1024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AVjxhjt4h_3"
      },
      "source": [
        "## Prediction of the model and converting to COCO format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 956, in entrypoint\n",
            "    model = YOLO(model, task=task)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/models/yolo/model.py\", line 79, in __init__\n",
            "    super().__init__(model=model, task=task, verbose=verbose)\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 151, in __init__\n",
            "    self._load(model, task=task)\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 295, in _load\n",
            "    self.model, self.ckpt = attempt_load_one_weight(weights)\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 1548, in attempt_load_one_weight\n",
            "    ckpt, weight = torch_safe_load(weight)  # load ckpt\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py\", line 1424, in torch_safe_load\n",
            "    file = attempt_download_asset(weight)  # search online if missing locally\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/utils/downloads.py\", line 471, in attempt_download_asset\n",
            "    tag, assets = get_github_assets(repo, release)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/utils/downloads.py\", line 419, in get_github_assets\n",
            "    r = requests.get(url)  # github api\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/requests/api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
            "    response = conn.getresponse()\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/brice/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-pac^C\n"
          ]
        }
      ],
      "source": [
        "# In this section, we run inference on validation images using the best model checkpoint.\n",
        "# This generates YOLO-format predictions in .txt files (1 per image).\n",
        "!yolo predict \\\n",
        "    model=runs/detect/yolo8x_cxr_v2_res1024_f2/weights/best.pt \\\n",
        "    source=yolo_cxr/images/val \\\n",
        "    save_txt=True \\\n",
        "    save_conf=True \\\n",
        "    project=runs/detect \\\n",
        "    name=yolo8x_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0OBu2uRHxlAU"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'runs/detect/yolo8x_cxr_v2_res1024_f2/predictions_native.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# We convert the YOLO .txt predictions to COCO-style JSON\u001b[39;00m\n\u001b[32m     46\u001b[39m PRED_NATIVE = \u001b[33m\"\u001b[39m\u001b[33mruns/detect/yolo8x_cxr_v2_res1024_f2/predictions_native.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43myolo_to_coco_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns/detect/yolo8x_pred\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPRED_NATIVE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCOCO-format predictions written to → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPRED_NATIVE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36myolo_to_coco_pred\u001b[39m\u001b[34m(txt_dir, output_json)\u001b[39m\n\u001b[32m     38\u001b[39m             pred_id += \u001b[32m1\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Write all predictions to a single COCO-style JSON file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     42\u001b[39m     json.dump(preds, f)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'runs/detect/yolo8x_cxr_v2_res1024_f2/predictions_native.json'"
          ]
        }
      ],
      "source": [
        "# Function to convert YOLO .txt predictions → COCO-style JSON like we had at the beginning\n",
        "def yolo_to_coco_pred(txt_dir, output_json):\n",
        "    preds = []\n",
        "    pred_id = 1  # Unique ID for each predicted instance\n",
        "\n",
        "    for img_id, (_, row) in enumerate(val_df.iterrows(), 1):\n",
        "        img_name = row[\"Image Index\"]\n",
        "        txt_path = Path(txt_dir) / \"labels\" / img_name.replace(\".png\", \".txt\")\n",
        "\n",
        "        if not txt_path.exists():\n",
        "            continue\n",
        "\n",
        "        w, h = Image.open(Path(IMAGES) / img_name).size\n",
        "\n",
        "        with open(txt_path) as f:\n",
        "            for line in f:\n",
        "                cls, xc, yc, bw, bh, *conf = map(float, line.split())\n",
        "                conf = (\n",
        "                    conf[0] if conf else 0.5\n",
        "                )  # Default confidence if missing\n",
        "                x = (xc - bw / 2) * w\n",
        "                y = (yc - bh / 2) * h\n",
        "\n",
        "                preds.append(\n",
        "                    {\n",
        "                        \"id\": pred_id,\n",
        "                        \"image_id\": img_id,\n",
        "                        \"category_id\": int(cls) + 1,  # COCO IDs start at 1\n",
        "                        \"bbox\": [\n",
        "                            x,\n",
        "                            y,\n",
        "                            bw * w,\n",
        "                            bh * h,\n",
        "                        ],  # Format: [x_min, y_min, width, height]\n",
        "                        \"score\": conf,\n",
        "                    }\n",
        "                )\n",
        "                pred_id += 1\n",
        "\n",
        "    # Write all predictions to a single COCO-style JSON file\n",
        "    with open(output_json, \"w\") as f:\n",
        "        json.dump(preds, f)\n",
        "\n",
        "\n",
        "# We convert the YOLO .txt predictions to COCO-style JSON\n",
        "PRED_NATIVE = \"runs/detect/yolo8x_cxr_v2_res1024_f2/predictions_native.json\"\n",
        "yolo_to_coco_pred(\"runs/detect/yolo8x_pred\", PRED_NATIVE)\n",
        "\n",
        "print(f\"COCO-format predictions written to → {PRED_NATIVE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV8rbCMM4o1p"
      },
      "source": [
        "## Creating the submission.csv file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFwxuyg1zWDV"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'runs/detect/yolo8x_cxr_v2_res1024_f2/weights/best.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Here we build submission.csv from test predictions\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Loading the fine-tuned model\u001b[39;00m\n\u001b[32m      4\u001b[39m MODEL_PATH = \u001b[33m\"\u001b[39m\u001b[33mruns/detect/yolo8x_cxr_v2_res1024_f2/weights/best.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Here, we read the ID ↔ image mapping\u001b[39;00m\n\u001b[32m      8\u001b[39m mapping = pd.read_csv(\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/content/drive/MyDrive/M2/M2-UES/Analyse_images/m-2-iasd-app-dlia-project-2025/ID_to_Image_Mapping.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/models/yolo/model.py:79\u001b[39m, in \u001b[36mYOLO.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m = new_instance.\u001b[34m__dict__\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRTDETR\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model[-\u001b[32m1\u001b[39m]._get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:151\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28mself\u001b[39m._new(model, task=task, verbose=verbose)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:295\u001b[39m, in \u001b[36mModel._load\u001b[39m\u001b[34m(self, weights, task)\u001b[39m\n\u001b[32m    292\u001b[39m weights = checks.check_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights).rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.ckpt = \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.task = \u001b[38;5;28mself\u001b[39m.model.task\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m.overrides = \u001b[38;5;28mself\u001b[39m.model.args = \u001b[38;5;28mself\u001b[39m._reset_ckpt_args(\u001b[38;5;28mself\u001b[39m.model.args)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:1548\u001b[39m, in \u001b[36mattempt_load_one_weight\u001b[39m\u001b[34m(weight, device, inplace, fuse)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mattempt_load_one_weight\u001b[39m(weight, device=\u001b[38;5;28;01mNone\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m, fuse=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1535\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1536\u001b[39m \u001b[33;03m    Load a single model weights.\u001b[39;00m\n\u001b[32m   1537\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1546\u001b[39m \u001b[33;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[32m   1547\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1548\u001b[39m     ckpt, weight = \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1549\u001b[39m     args = {**DEFAULT_CFG_DICT, **(ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[32m   1550\u001b[39m     model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]).to(device).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:1446\u001b[39m, in \u001b[36mtorch_safe_load\u001b[39m\u001b[34m(weight, safe_only)\u001b[39m\n\u001b[32m   1444\u001b[39m                 ckpt = torch.load(f, pickle_module=safe_pickle)\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m             ckpt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.name == \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/ultralytics/utils/patches.py:119\u001b[39m, in \u001b[36mtorch_load\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    117\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PERSONAL_PROJECT/Chest_X-ray_Detection_Challenge/.venv/lib/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'runs/detect/yolo8x_cxr_v2_res1024_f2/weights/best.pt'"
          ]
        }
      ],
      "source": [
        "# Here we build submission.csv from test predictions\n",
        "\n",
        "# Loading the fine-tuned model\n",
        "MODEL_PATH = \"runs/detect/yolo8x_cxr_v2_res1024_f2/weights/best.pt\"\n",
        "model = YOLO(MODEL_PATH)\n",
        "\n",
        "# Here, we read the ID ↔ image mapping\n",
        "mapping = pd.read_csv(\n",
        "    os.path.join(current_dir, \"data\", \"ID_to_Image_Mapping.csv\")\n",
        ")\n",
        "\n",
        "\n",
        "TEST_DIR = os.path.join(current_dir, \"test\")\n",
        "image_files = {f for f in os.listdir(TEST_DIR) if f.endswith(\".png\")}\n",
        "mapping = mapping[mapping[\"image_id\"].isin(image_files)].copy()\n",
        "\n",
        "\n",
        "# We add a rank column (0, 1, 2, …) to distinguish multiple boxes per image\n",
        "mapping[\"rank\"] = mapping.groupby(\"image_id\").cumcount()\n",
        "\n",
        "# And run inference on the test set\n",
        "results = model.predict(\n",
        "    source=TEST_DIR,\n",
        "    imgsz=1024,\n",
        "    conf=0.0,  # we keep all boxes we’ll filter later\n",
        "    save=False,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# In this below section,\n",
        "# we re-format predictions and assign a rank based on confidence\n",
        "pred_records = []\n",
        "\n",
        "for res in results:\n",
        "    img_name = os.path.basename(res.path)\n",
        "\n",
        "    confs = res.boxes.conf.cpu().numpy()\n",
        "    order = confs.argsort()[::-1]  # we order them by descending confidence\n",
        "\n",
        "    boxes = res.boxes.xyxy.cpu().numpy()\n",
        "    clss = res.boxes.cls.cpu().numpy()\n",
        "\n",
        "    for rank, idx in enumerate(order):\n",
        "        x_min, y_min, x_max, y_max = boxes[idx]\n",
        "        pred_records.append(\n",
        "            {\n",
        "                \"image_id\": img_name,\n",
        "                \"rank\": rank,\n",
        "                \"x_min\": float(x_min),\n",
        "                \"y_min\": float(y_min),\n",
        "                \"x_max\": float(x_max),\n",
        "                \"y_max\": float(y_max),\n",
        "                \"confidence\": float(confs[idx]),\n",
        "                \"label\": model.names[int(clss[idx])],\n",
        "            }\n",
        "        )\n",
        "\n",
        "preds_df = pd.DataFrame(pred_records)\n",
        "\n",
        "#  Merge mapping and predictions on (image_id, rank)\n",
        "submission_df = mapping.merge(\n",
        "    preds_df,\n",
        "    on=[\"image_id\", \"rank\"],\n",
        "    how=\"left\",\n",
        "    validate=\"one_to_one\",\n",
        "    # every mapping row must find exactly one box\n",
        "    # (submission.csv format requirements)\n",
        ")\n",
        "\n",
        "assert not submission_df[\"id\"].isna().any(), \"Missing IDs in submission\"\n",
        "assert not submission_df[\"confidence\"].isna().any(), (\n",
        "    \"Missing boxes for some IDs\"\n",
        ")\n",
        "\n",
        "# Select final columns and export to CSV\n",
        "submission_df = submission_df[\n",
        "    [\n",
        "        \"id\",\n",
        "        \"image_id\",\n",
        "        \"x_min\",\n",
        "        \"y_min\",\n",
        "        \"x_max\",\n",
        "        \"y_max\",\n",
        "        \"confidence\",\n",
        "        \"label\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "OUT_PATH = os.path.join(current_dir, \"/outputs/submission.csv\")\n",
        "submission_df.to_csv(OUT_PATH, index=False)\n",
        "\n",
        "print(f\"submission.csv saved – {len(submission_df)} rows (all IDs unique).\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wde4FLAKW_NH",
        "TQOYipF9YoXV",
        "0xSU8cSSe0Kk",
        "nGjYzpdve01e",
        "MVVryHMR4J8h",
        "YNUPV_wk4UcI",
        "ZThjNgfm4Z8Q",
        "Paifufz14fHv",
        "9AVjxhjt4h_3",
        "DV8rbCMM4o1p"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "chestxrayannotations",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
